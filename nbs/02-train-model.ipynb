{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(pl.LightningDataModule):\n",
    "    def __init__(self, data=None, scaler=None):\n",
    "        super(dataset,self).__init__()\n",
    "        self.lookback_size = 5\n",
    "        self.batch_size = 32\n",
    "\n",
    "        if data is not None:\n",
    "            self.data=data\n",
    "            self.series = pd.read_csv(self.data, index_col='Date')\n",
    "            # Train:valid:test = 80:10:10\n",
    "            self.train_df, self.valid_df = train_test_split(self.series, test_size=0.2)\n",
    "            self.valid_df, self.test_df = train_test_split(self.valid_df, test_size=0.5)\n",
    "\n",
    "        if scaler is None and data is not None:\n",
    "            self.scaler = MinMaxScaler().fit(self.train_df)\n",
    "        else:\n",
    "            self.scaler=scaler\n",
    "\n",
    "    def train_tensors(self,df):\n",
    "\n",
    "        X, y = [], []\n",
    "\n",
    "        for i in np.arange(self.lookback_size, len(df)-1):\n",
    "            X.append(df[i-self.lookback_size:i])\n",
    "            y.append(df[i+1])\n",
    "\n",
    "        X = np.array(X).reshape(-1,self.lookback_size,1)\n",
    "        y = np.array(y).reshape(-1,1)\n",
    "        return TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        self.train_df = self.scaler.transform(self.train_df) \n",
    "        self.valid_df = self.scaler.transform(self.valid_df) \n",
    "        self.test_df = self.scaler.transform(self.test_df) \n",
    "\n",
    "        self.train_data = self.train_tensors(self.train_df)\n",
    "        self.valid_data = self.train_tensors(self.valid_df)\n",
    "        self.test_data = self.train_tensors(self.test_df)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "       return DataLoader(self.valid_data, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "       return DataLoader(self.test_data, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,lookback_size=5):\n",
    "\n",
    "        super(model,self).__init__()\n",
    "\n",
    "        self.lookback_size = lookback_size\n",
    "        self.lstm=torch.nn.LSTM(batch_first=True, input_size=1, hidden_size=self.lookback_size)\n",
    "        self.out=torch.nn.Linear(5,1)\n",
    "        self.loss=torch.nn.functional.mse_loss\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x, hidden = self.lstm(x)\n",
    "        x = x[:,-1]\n",
    "        x = self.out(x)\n",
    "        return x, hidden\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params=self.parameters(), lr=1e-3)\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "        checkpoint = ModelCheckpoint(monitor=\"val_loss\")\n",
    "        return [early_stop, checkpoint]\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch \n",
    "        logits,_ = self.forward(x.type(torch.float32)) \n",
    "        loss = self.loss(logits.float(), y.float()) \n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, valid_batch, batch_idx): \n",
    "        x, y = valid_batch \n",
    "        logits,_ = self.forward(x.type(torch.float32)) \n",
    "        loss = self.loss(logits.float(), y.float())\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx): \n",
    "        x, y = test_batch \n",
    "        logits,_ = self.forward(x.type(torch.float32)) \n",
    "        loss = self.loss(logits.float(), y.float())\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path):\n",
    "    trainer=pl.Trainer(max_epochs=5, enable_progress_bar=True)\n",
    "    mlflow.pytorch.autolog()\n",
    "    datamod=dataset(path)\n",
    "    mod=model()\n",
    "    with mlflow.start_run() as run:\n",
    "        trainer.fit(model=mod, datamodule=datamod)\n",
    "\n",
    "    trainer.test(model=mod, datamodule=datamod)\n",
    "\n",
    "    return mod, datamod.scaler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\Anaconda\\envs\\ak-mlops-az-gh-env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "2023/05/04 13:22:12 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n",
      "2023/05/04 13:22:16 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"d:\\Anaconda\\envs\\ak-mlops-az-gh-env\\lib\\site-packages\\mlflow\\pytorch\\_lightning_autolog.py:352: UserWarning: Autologging is known to be compatible with pytorch-lightning versions between 1.0.5 and 2.0.1.post0 and may not succeed with packages outside this range.\"\n",
      "INFO: The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "INFO:lightning.pytorch.utilities.rank_zero:The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "Missing logger folder: e:\\ak_mlops-az-gh\\nbs\\lightning_logs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | lstm | LSTM   | 160   \n",
      "1 | out  | Linear | 6     \n",
      "--------------------------------\n",
      "166       Trainable params\n",
      "0         Non-trainable params\n",
      "166       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\ak-mlops-az-gh-env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\ak-mlops-az-gh-env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "d:\\Anaconda\\envs\\ak-mlops-az-gh-env\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 6/6 [00:00<00:00, 115.69it/s, v_num=0, train_loss_step=0.214, val_loss=0.0971, train_loss_epoch=0.149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 6/6 [00:00<00:00, 97.03it/s, v_num=0, train_loss_step=0.214, val_loss=0.0971, train_loss_epoch=0.149] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/04 13:22:30 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"d:\\Anaconda\\envs\\ak-mlops-az-gh-env\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n",
      "INFO: The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: EarlyStopping, ModelCheckpoint\n",
      "INFO:lightning.pytorch.utilities.rank_zero:The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: EarlyStopping, ModelCheckpoint\n",
      "d:\\Anaconda\\envs\\ak-mlops-az-gh-env\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\ak-mlops-az-gh-env\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\ak-mlops-az-gh-env\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\ak-mlops-az-gh-env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 35.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     5.66511869430542      </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    5.66511869430542     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainedModel, scalerObj = train(\"../data/WIPRO.NS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scalerObj, open(r\"E:\\ak_mlops-az-gh\\outputs\\scaler.pkl\",\"wb\"))\n",
    "model_file = r\"E:\\ak_mlops-az-gh\\outputs\\model.pth\"\n",
    "torch.save(trainedModel.state_dict(), model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ak-mlops-az-gh-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
