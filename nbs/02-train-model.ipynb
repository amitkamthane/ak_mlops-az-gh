{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39margparse\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_absolute_percentage_error\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(pl.LightningDataModule):\n",
    "    def __init__(self, data=None, scaler=None):\n",
    "        super(dataset,self).__init__()\n",
    "        self.lookback_size = 5\n",
    "        self.batch_size = 32\n",
    "\n",
    "        if data is not None:\n",
    "            self.data=data\n",
    "            self.series = pd.read_csv(self.data, index_col='Date')\n",
    "            # Train:valid:test = 80:10:10\n",
    "            self.train_df, self.valid_df = train_test_split(self.series, test_size=0.2)\n",
    "            self.valid_df, self.test_df = train_test_split(self.valid_df, test_size=0.5)\n",
    "\n",
    "        if scaler is None and data is not None:\n",
    "            self.scaler = MinMaxScaler().fit(self.train_df)\n",
    "        else:\n",
    "            self.scaler=scaler\n",
    "\n",
    "    def train_tensors(self,df):\n",
    "\n",
    "        X, y = [], []\n",
    "\n",
    "        for i in np.arange(self.lookback_size, len(df)-1):\n",
    "            X.append(df[i-self.lookback_size:i])\n",
    "            y.append(df[i+1])\n",
    "\n",
    "        X = np.array(X).reshape(-1,self.lookback_size,1)\n",
    "        y = np.array(y).reshape(-1,1)\n",
    "        return TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        self.train_df = self.scaler.transform(self.train_df) \n",
    "        self.valid_df = self.scaler.transform(self.valid_df) \n",
    "        self.test_df = self.scaler.transform(self.test_df) \n",
    "\n",
    "        self.train_data = self.train_tensors(self.train_df)\n",
    "        self.valid_data = self.train_tensors(self.valid_df)\n",
    "        self.test_data = self.train_tensors(self.test_df)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "       return DataLoader(self.valid_data, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "       return DataLoader(self.test_data, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import mlflow.pytorch\n",
    "class dataset(pl.LightningDataModule):\n",
    "    def __init__(self, data=None, scaler=None):\n",
    "        super(dataset,self).__init__()\n",
    "        self.lookback_size = 5\n",
    "        self.batch_size = 32\n",
    "\n",
    "        if data is not None:\n",
    "            self.data=data\n",
    "            self.series = pd.read_csv(self.data, index_col='Date')\n",
    "            # Train:valid:test = 80:10:10\n",
    "            self.train_df, self.valid_df = train_test_split(self.series, test_size=0.2)\n",
    "            self.valid_df, self.test_df = train_test_split(self.valid_df, test_size=0.5)\n",
    "\n",
    "        if scaler is None and data is not None:\n",
    "            self.scaler = MinMaxScaler().fit(self.train_df)\n",
    "        else:\n",
    "            self.scaler=scaler\n",
    "\n",
    "    def train_tensors(self,df):\n",
    "\n",
    "        X, y = [], []\n",
    "\n",
    "        for i in np.arange(self.lookback_size, len(df)-1):\n",
    "            X.append(df[i-self.lookback_size:i])\n",
    "            y.append(df[i+1])\n",
    "\n",
    "        X = np.array(X).reshape(-1,self.lookback_size,1)\n",
    "        y = np.array(y).reshape(-1,1)\n",
    "        return TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        self.train_df = self.scaler.transform(self.train_df) \n",
    "        self.valid_df = self.scaler.transform(self.valid_df) \n",
    "        self.test_df = self.scaler.transform(self.test_df) \n",
    "\n",
    "        self.train_data = self.train_tensors(self.train_df)\n",
    "        self.valid_data = self.train_tensors(self.valid_df)\n",
    "        self.test_data = self.train_tensors(self.test_df)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "       return DataLoader(self.valid_data, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "       return DataLoader(self.test_data, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,lookback_size=5):\n",
    "\n",
    "        super(model,self).__init__()\n",
    "\n",
    "        self.lookback_size = lookback_size\n",
    "        self.lstm=torch.nn.LSTM(batch_first=True, input_size=1, hidden_size=self.lookback_size)\n",
    "        self.out=torch.nn.Linear(5,1)\n",
    "        self.loss=torch.nn.functional.mse_loss\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x, hidden = self.lstm(x)\n",
    "        x = x[:,-1]\n",
    "        x = self.out(x)\n",
    "        return x, hidden\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params=self.parameters(), lr=1e-3)\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "        checkpoint = ModelCheckpoint(monitor=\"val_loss\")\n",
    "        return [early_stop, checkpoint]\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch \n",
    "        logits,_ = self.forward(x.type(torch.float32)) \n",
    "        loss = self.loss(logits.float(), y.float()) \n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, valid_batch, batch_idx): \n",
    "        x, y = valid_batch \n",
    "        logits,_ = self.forward(x.type(torch.float32)) \n",
    "        loss = self.loss(logits.float(), y.float())\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx): \n",
    "        x, y = test_batch \n",
    "        logits,_ = self.forward(x.type(torch.float32)) \n",
    "        loss = self.loss(logits.float(), y.float())\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path):\n",
    "    trainer=pl.Trainer(max_epochs=5, enable_progress_bar=True)\n",
    "    mlflow.pytorch.autolog()\n",
    "    datamod=dataset(path)\n",
    "    mod=model()\n",
    "    with mlflow.start_run() as run:\n",
    "        trainer.fit(model=mod, datamodule=datamod)\n",
    "\n",
    "    trainer.test(model=mod, datamodule=datamod)\n",
    "\n",
    "    return mod, datamod.scaler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel, scalerObj = train(\"../data/WIPRO.NS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "┘\n",
    "pickle.dump(scalerObj, open(r\"E:\\ak_mlops-az-gh\\outputs\\scaler.pkl\",\"wb\"))\n",
    "model_file = f\"E:\\mlops-az-gh\\outputs\\model.pth\"\n",
    "torch.save(trainedModel.state_dict(), model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ak-mlops-az-gh-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
